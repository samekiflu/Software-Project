# Model Evaluation System

A comprehensive system for evaluating machine learning models, datasets, and code repositories across multiple quality metrics.

---

## Overview
This system evaluates AI/ML models from various sources (HuggingFace, GitHub) and provides standardized quality scores based on multiple criteria including license compatibility, performance claims, code quality, dataset quality, and more.

---

## Features
- **Multi-source Support**: Evaluates models from HuggingFace, datasets from HuggingFace, and code repositories from GitHub  
- **Comprehensive Metrics**: 8 different quality metrics with weighted scoring  
- **Parallel Processing**: Concurrent evaluation of multiple metrics for performance  
- **Flexible Input**: Process URLs from files with support for grouped evaluations  
- **Logging Support**: Configurable logging levels with file output  
- **Test Suite**: 24 comprehensive test cases with 85%+ code coverage  

---

## Installation

### Prerequisites
- Python **3.7+**
- Internet connection for API access

### Install Dependencies
./run install

This installs:
- requests â€“ For API communications  
- typing-extensions â€“ For enhanced type hints  
- coverage â€“ For test coverage analysis  

---

## Usage

### Basic Usage
./run absolute directory of txt file        # Evaluate URLs from a file  
./run install         # Install dependencies  
./run test            # Run test suite  

---

## URL File Format
Create a text file with URLs, one group per line (comma-separated):

https://github.com/google-research/bert, https://huggingface.co/datasets/bookcorpus/bookcorpus, https://huggingface.co/google-bert/bert-base-uncased  
,,https://huggingface.co/parvk11/audience_classifier_model  
,,https://huggingface.co/openai/whisper-tiny  

---

## ğŸ”§ Environment Variables

### Logging Configuration
- LOG_LEVEL: Controls logging verbosity  
  - 0 â€“ Silent (no logs)  
  - 1 â€“ INFO level  
  - 2 â€“ DEBUG level  
- LOG_FILE: Path to log file (optional, defaults to console)  

---

## ğŸ“Š Metrics

The system evaluates models using 8 weighted metrics:

License (20%) â€“ License compatibility (MIT, Apache, BSD, GPL, etc.)  
Performance Claims (15%) â€“ Presence of benchmark results and performance data  
Ramp-up Time (15%) â€“ Documentation quality and ease of use  
Bus Factor (10%) â€“ Number of contributors and project sustainability  
Size Score (10%) â€“ Model size compatibility across hardware platforms  
Dataset & Code Score (10%) â€“ Availability of training data and source code  
Dataset Quality (10%) â€“ Quality metrics for associated datasets  
Code Quality (10%) â€“ Code repository quality and maintenance  

Net Score Calculation:
Net Score = Î£(metric_score Ã— weight) / Î£(weights)  

---

## ğŸ“‘ Output Format
Results are returned in NDJSON format:

{
  "name": "bert-base-uncased",
  "category": "MODEL",
  "net_score": 0.797,
  "net_score_latency": 820,
  "ramp_up_time": 0.5,
  "ramp_up_time_latency": 176,
  "bus_factor": 1.0,
  "bus_factor_latency": 171,
  "performance_claims": 0.5,
  "performance_claims_latency": 103,
  "license": 1.0,
  "license_latency": 193,
  "size_score": {
    "raspberry_pi": 0.0,
    "jetson_nano": 0.3,
    "desktop_pc": 1.0,
    "aws_server": 1.0
  },
  "size_score_latency": 177,
  "dataset_and_code_score": 1.0,
  "dataset_and_code_score_latency": 0,
  "dataset_quality": 1.0,
  "dataset_quality_latency": 0,
  "code_quality": 0.6,
  "code_quality_latency": 216
}

---

## ğŸ—ï¸ Architecture

### Project Structure
â”œâ”€â”€ run                     # Main entry point script  
â”œâ”€â”€ model_evaluator.py      # Core evaluation orchestrator  
â”œâ”€â”€ url_classifier.py       # URL type classification  
â”œâ”€â”€ handlers/               # Resource-specific handlers  
â”‚   â”œâ”€â”€ __init__.py  
â”‚   â”œâ”€â”€ base_resource_handler.py  
â”‚   â”œâ”€â”€ model_handler.py    # HuggingFace model handling  
â”‚   â”œâ”€â”€ dataset_handler.py  # HuggingFace dataset handling  
â”‚   â””â”€â”€ code_handler.py     # GitHub repository handling  
â”œâ”€â”€ metrics/                # Evaluation metrics  
â”‚   â”œâ”€â”€ __init__.py  
â”‚   â”œâ”€â”€ base_metric.py  
â”‚   â”œâ”€â”€ license_metric.py  
â”‚   â”œâ”€â”€ size_score_metric.py  
â”‚   â”œâ”€â”€ ramp_up_time_metric.py  
â”‚   â”œâ”€â”€ bus_factor_metric.py  
â”‚   â”œâ”€â”€ performance_claims_metric.py  
â”‚   â”œâ”€â”€ dataset_and_code_score_metric.py  
â”‚   â”œâ”€â”€ dataset_quality_metric.py  
â”‚   â””â”€â”€ code_quality_metric.py  
â”œâ”€â”€ test_model_evaluator.py # Comprehensive test suite  
â””â”€â”€ requirements.txt        # Python dependencies  

---

## Key Components
- ModelEvaluator: Main orchestrator that coordinates evaluation  
- URLClassifier: Identifies URL types (MODEL, DATASET, CODE, UNKNOWN)  
- Resource Handlers: Specialized handlers for each platform/type  
- Metrics: Individual metric calculators with parallel execution  
- Logging System: Configurable logging with file output support  

---

## Testing
Run the comprehensive test suite:
./run test  

Coverage: 85%+ line coverage across core modules  
Test Cases: 24 tests covering:  
- URL classification  
- Resource handler functionality  
- Metric calculations  
- Error handling  
- Logging configuration  
- File processing  

---

## Supported Platforms

### Input Sources
- HuggingFace Models: https://huggingface.co/[org]/[model]  
- HuggingFace Datasets: https://huggingface.co/datasets/[org]/[dataset]  
- GitHub Repositories: https://github.com/[org]/[repo]  

### License Support
- MIT  
- Apache 2.0  
- BSD (2-clause, 3-clause)  
- GPL (v2, v3)  
- LGPL (v2.1, v3)  
- Creative Commons Zero (CC0)  
- Unlicense / Public Domain  

### Hardware Platforms (Size Scoring)
- Raspberry Pi  
- Jetson Nano  
- Desktop PC  
- AWS Server  

---

## Error Handling
The system includes robust error handling for:  
- Invalid URLs  
- Network timeouts  
- API rate limits  
- Invalid file paths  
- Missing dependencies  
- Authentication failures  

---

## ğŸ“„ License
This project is for educational purposes as part of a software engineering course.
